# Mahalanobis Distance Analysis on Breast Cancer Data

## üìù Project Overview

This project compares three different distance and similarity metrics - **Euclidean distance**, **Cosine similarity**, and **Mahalanobis distance** - to analyze their effectiveness in differentiating between data instances in the Breast Cancer Wisconsin (Original) dataset. The goal is to see which metric's output best correlates with the classification of the instances (benign vs. malignant).

The analysis focuses on five specific data points from the dataset:
* d1: id=1035283
* d2: id=529329
* d3: id=1017122
* d4: id=1185609
* d5: id=1230175

## üìä Dataset

The data used is the **Breast Cancer Wisconsin (Original) Dataset** from the UCI Machine Learning Repository.

A cleaned version of the dataset is generated by the `reading and storing data.py` script and saved as `HW1-win-data.csv`.

## ‚öôÔ∏è How to Run

1.  **Prerequisites**: Ensure you have Python and the following libraries installed:
    * `pandas`
    * `numpy`
    * `scikit-learn`
    * `scipy`

2.  **Data Cleaning**: Run the data preparation script first to generate the clean CSV file.
    ```bash
    python "reading and storing data.py"
    ```

3.  **Run Analysis**: Execute the main script to calculate the metrics and print the results.
    ```bash
    python main.py
    ```

## üìà Results and Analysis

The following matrices were calculated using the feature attributes of the five selected data points. The class attribute was intentionally excluded to see if the metrics could independently group similar instances.

### Euclidean Distance

The Euclidean distance measures the direct "as-the-crow-flies" distance between points.

**Distance Matrix:**
| | d1 | d2 | d3 | d4 | d5 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **d1** | 0.0 | 21.19 | 12.45 | 13.71 | 6.86 |
| **d2** | 21.19 | 0.0 | 9.11 | 24.02 | 27.93 |
| **d3** | 12.45 | 9.11 | 0.0 | 14.0 | 18.68 |
| **d4** | 13.71 | 24.02 | 14.0 | 0.0 | 11.22 |
| **d5** | 6.86 | 27.93 | 18.68 | 11.22 | 0.0 |

**Analysis:**
- A clear trend is visible: smaller distances often correspond to instances with the same class, while larger distances correspond to different classes.
- For point **d1** (class 4), the nearest point is **d5** (class 4) with a distance of **6.86**, while the farthest is **d2** (class 2) with a distance of **21.19**.
- This suggests a decision boundary for class similarity exists somewhere between a distance of 12.45 and 13.71.
- **Conclusion**: Euclidean distance appears to be the most effective metric for this dataset, as it provides a strong signal for differentiating between classes based on feature similarity.

### Cosine Similarity

Cosine similarity measures the angle between two vectors, indicating directional similarity. A value of 1 means identical direction, while 0 means they are orthogonal.

**Similarity Matrix:**
| | d1 | d2 | d3 | d4 | d5 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **d1** | 1.0 | 0.77 | 0.88 | 0.96 | 0.91 |
| **d2** | 0.77 | 1.0 | 0.75 | 0.61 | 0.52 |
| **d3** | 0.88 | 0.75 | 1.0 | 0.82 | 0.75 |
| **d4** | 0.96 | 0.61 | 0.82 | 1.0 | 0.94 |
| **d5** | 0.91 | 0.52 | 0.75 | 0.94 | 1.0 |

**Analysis:**
- **d1** and **d4** have the highest similarity (**0.96**), yet they belong to different classes. Conversely, **d2** and **d3** have one of the lowest similarities (**0.75**) but share the same class.
- The similarity scores do not appear to correlate reliably with the class labels.
- **Conclusion**: Cosine similarity is not a useful metric for distinguishing between instances in this particular dataset.

### Mahalanobis Distance

This metric measures the distance between a point and a distribution, taking into account the correlations between the variables.

**Distance Matrix:**
| | d1 | d2 | d3 | d4 | d5 |
| :--- | :--- |:--- |:--- |:--- |:--- |
| **d1 vs d2** | 2.50 |
| **d1 vs d3** | 2.54 |
| **d1 vs d4** | 2.54 |
| **d1 vs d5** | 2.76 |
| **d2 vs d3** | 2.11 |
| **d2 vs d4** | 3.32 |
| **d2 vs d5** | 3.99 |
| **d3 vs d4** | 2.72 |
| **d3 vs d5** | 3.48 |
| **d4 vs d5** | 3.55 |

**Analysis:**
- The distances between **d1, d2, d3, and d4** are all relatively similar (ranging from ~2.11 to ~3.32).
- However, point **d5** is consistently placed far apart from all other instances, with distances ranging from **2.76 to 3.99**.
- This suggests that **d5** may be an outlier relative to the overall data distribution, but the metric does not clearly separate the other four points from each other in a meaningful way related to their class.
- **Conclusion**: Mahalanobis distance is effective at identifying that **d5** behaves differently from the central cluster of data, but it fails to provide clear separation among the other instances.

## Further Discussion
For this problem, considering data instances from this [dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29) with id-numbers as follows: a.d3: id=1017122, row-6b.d1: id=1035283, row-11c.d4: id=1185609, row-147d.d5: id=1230175, row=230e.d2: id=529329,  row-287

Note: The cleaned dataset has already been uploaded in this repository.
### Euclidean-distance
![Euclidean Distance Matrix](https://github.com/anij715/Mahalanobis-distance-analysis/blob/main/Euclid.PNG)
- From the above table, we can observe that, for the point D1, D2 is at the maximum distance (21.19) when both have different class values (D1.class == 4, D2.class == 2). While D5 is located the nearest, at 6.86 units from D1, both instances have the same class values (~4). Similarly, for other points, this trend is being followed by each pair. 
- Also, for distance of 12.45 units, class value is same, while for 13.71 units, class value is different, hence it has been noticed that the deciding value of distance between two instance points lies somewhere between 12.45 and 13.11.
- Therefore, we can say that, for distance less than 12.45, class values will be same, and for distance greater than 13.71, class values will be different. And, with the increase in Euclidean distance, probability of the class attributes being different also increases. 

- Except for the ‚ÄòMitoses‚Äô & ‚ÄòUniformity of Cell Size‚Äô, we can differentiate the other attributes of these instances based on the Euclidean distance from the above table. Even though we have not included the class attribute while calculating the Euclidean distance, we have a relational dependence, means the other attributes also directly influences the class attribute, for e.g., nucleoli, if the value is large, there is a high chance that the class value will also be 4.
### Cosine-similarity
![Euclidean Distance Matrix](https://github.com/anij715/Mahalanobis-distance-analysis/blob/main/Cosine.PNG)
- Here, D2 & D3 have the lowest cosine similarity amongst all pairs, while D1 and D4 have the highest similarity. D2 & D3 have the same class values and D1 & D4 have the different class values. Also, while calculating the cosine similarity, we have not included the class attribute. So, from the above observations, I think we cannot determine different  instances with the help of cosine similarity.
### Mahalanobis-distance
![Euclidean Distance Matrix](https://github.com/anij715/Mahalanobis-distance-analysis/blob/main/MAHA.PNG)
- Here, D5 & D4 are the most apart based on the mahalanobis distance, and D1 & D2 are least separated amongst all pairs. Now consider D1, it has almost similar values of mahalanobis distance from D1, D3, and D4. And the only attribute which is same among these four instances is ‚ÄúMitosis‚Äù with value 1. Now, for D2, the mahalanobis distance is similar from D1 & D3, but not from D4 and D5. Also, comparatively, D5 is placed far apart from each instance.

- So, it can be said that D1, D2, D3 & D4 all lie in the cartesian plane not too empty, but D5 on the other hand is very far from the cluster. From this, we cannot be certain about the class values of the D1, D2, D3, or D4, but we can say that D5 lies will have different behavior than the others.

## üèÜ Final Conclusion

Based on this analysis, **Euclidean distance is the best-suited metric for this dataset**. It demonstrates a strong and interpretable relationship between the distance value and the likelihood that two instances belong to the same class. In contrast, Cosine similarity provided no clear pattern, and Mahalanobis distance was only useful for identifying a single potential outlier.
